{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "# !pip install openml\n",
    "import openml\n",
    "# !pip install cardinal\n",
    "# !pip install scikit-learn==0.20.4\n",
    "# !pip install sklearn.cluster._k_means_fast\n",
    "from cardinal.uncertainty import MarginSampler\n",
    "from cardinal.random import RandomSampler\n",
    "from cardinal.zhdanov2019 import TwoStepKMeansSampler\n",
    "from cardinal.plotting import plot_confidence_interval\n",
    "import tqdm\n",
    "\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import clone_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Détermination manuelle des features catégorielles OpenML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ids = [1471, 1502, 40922, 41162, 43551, 1461, 1590, 41138, 42395, 42803, 43439] \n",
    "#DONE : 1471, 1502, 40922  (arrays ?)     \\   41162, 43551, 1461, 1590, 41138, 42395, 42803, 43439 \n",
    "\n",
    "# Maybe : 42088\n",
    "#TODO : 42750\n",
    "#No : 42493, 42256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(dataset_id):\n",
    "    \"\"\"\n",
    "    Returns X, y corresponding to a specific OpenML dataset id with some additional preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = openml.datasets.get_dataset(dataset_id)\n",
    "\n",
    "    if dataset_id in [1471, 1502, 40922,    1461, 1590, 41138]:\n",
    "        X, y, cat_indicator, names = dataset.get_data(dataset_format='array', target=dataset.default_target_attribute)  #dataframe / array\n",
    "    else:\n",
    "        X, y, cat_indicator, names = dataset.get_data(dataset_format='dataframe', target=dataset.default_target_attribute)  #dataframe / array\n",
    "        y = np.asarray(y)   #Update here in order to easily access the labels with kfolds (array indexing)\n",
    "    \n",
    "    cat_indicator = np.asarray(cat_indicator)\n",
    "\n",
    "\n",
    "    #Special preprocessing for debugging\n",
    "    if dataset_id == 42395:\n",
    "        X=X.drop(['ID_code'], axis = 1)       #id = 42395\n",
    "        cat_indicator = cat_indicator[1:] #TODO\n",
    "    if dataset_id == 42088:\n",
    "        X=X.drop(['brewery_name', 'review_profilename'], axis = 1)        #id = 42088\n",
    "        cat_indicator = cat_indicator[2:] #TODO\n",
    "    if dataset_id == 42256:\n",
    "        X=X.drop(['full_name'], axis = 1)       #id = 42256\n",
    "        cat_indicator = cat_indicator[1:] #TODO\n",
    "    if dataset_id == 42803:\n",
    "        X=X.drop(['Accident_Index', 'Date','Time', 'Local_Authority_(Highway)', 'LSOA_of_Accident_Location'], axis = 1)       #id = 42803\n",
    "        cat_indicator = cat_indicator[5:] #TODO\n",
    "    if dataset_id == 43439:\n",
    "        X=X.drop(['Gender', 'ScheduledDay', 'AppointmentDay','Neighbourhood'], axis = 1)     \n",
    "        cat_indicator = cat_indicator[4:] #TODO\n",
    "    if dataset_id == 42088: \n",
    "        X=X.drop(['brewery_name', 'review_profilename', 'beer_name'], axis = 1)      #id = 42088\n",
    "        cat_indicator = cat_indicator[3:] #TODO\n",
    "\n",
    "\n",
    "    ct_cat = ColumnTransformer([\n",
    "        ('normalizer', StandardScaler(), np.where(~cat_indicator)[0])\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "    X_cat = pd.DataFrame(ct_cat.fit_transform(X)).convert_dtypes()\n",
    "\n",
    "    ct = ColumnTransformer([\n",
    "        ('encoder', OneHotEncoder(), np.where(cat_indicator)[0]),\n",
    "        ('normalizer', StandardScaler(), np.where(~cat_indicator)[0])\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "    X = ct.fit_transform(X)\n",
    "\n",
    "    #TODO\n",
    "\n",
    "    if dataset_id in [41162, 1590]:\n",
    "        X = np.asarray(np.nan_to_num(X.todense()))    #X.todense()\n",
    "    else:\n",
    "        X = np.asarray(np.nan_to_num(X))\n",
    "\n",
    "    #Shuffle\n",
    "    # if dataset_id in [1471, 1502, 40922]:\n",
    "    #     idx = np.arange(len(X)) \n",
    "    # else:\n",
    "    # print(X.shape)\n",
    "    idx = np.arange(X.shape[0]) \n",
    "    np.random.shuffle(idx)\n",
    "    X = X[idx]\n",
    "    y = y[idx]\n",
    "    \n",
    "    #Reduce execution time\n",
    "    X = X[:int(0.1 * X.shape[0])]\n",
    "    y = y[:int(0.1 * y.shape[0])]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# X.shape, y.shape, batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "models dataset id=1471: 100%|██████████| 1/1 [00:00<00:00, 3184.74it/s]\n",
      "models dataset id=1502: 100%|██████████| 1/1 [00:00<00:00, 7037.42it/s]\n",
      "models dataset id=40922: 100%|██████████| 1/1 [00:00<00:00, 2308.37it/s]\n",
      "models dataset id=41162: 100%|██████████| 1/1 [00:00<00:00, 3125.41it/s]\n",
      "models dataset id=43551: 100%|██████████| 1/1 [00:00<00:00, 9279.43it/s]\n",
      "models dataset id=1461: 100%|██████████| 1/1 [00:00<00:00, 4462.03it/s]\n",
      "models dataset id=1590: 100%|██████████| 1/1 [00:00<00:00, 3953.16it/s]\n",
      "models dataset id=41138: 100%|██████████| 1/1 [00:00<00:00, 3334.10it/s]\n",
      "models dataset id=42395: 100%|██████████| 1/1 [00:00<00:00, 4100.00it/s]\n",
      "models dataset id=42803: 100%|██████████| 1/1 [00:00<00:00, 7681.88it/s]\n",
      "models dataset id=43439: 100%|██████████| 1/1 [00:00<00:00, 5405.03it/s]\n",
      "DATASETS: 100%|██████████| 11/11 [00:20<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    ('GBC', GradientBoostingClassifier()),\n",
    "    # ('Margin', MarginSampler(model, batch_size)),\n",
    "    # ('Random', RandomSampler(batch_size)),\n",
    "]\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "num_folds = 10\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "skf = StratifiedKFold(n_splits=num_folds)\n",
    "\n",
    "\n",
    "for dataset_id in tqdm.tqdm(dataset_ids, desc = 'DATASETS'):\n",
    "\n",
    "    # print(dataset_id)\n",
    "\n",
    "    X, y = getData(dataset_id)\n",
    "\n",
    "    for model_name, base_model in tqdm.tqdm(models, desc = f'models dataset id={dataset_id}'):\n",
    "\n",
    "        #Check if model already studied in this dataset\n",
    "        filePath = f'./results/{dataset_id}-{model_name}.csv'\n",
    "        if os.path.isfile(filePath) == False :\n",
    "\n",
    "            all_accuracies = []\n",
    "\n",
    "            #Train/test split : le \"test set\" sera utilisé plus tard dans le benchmark pour entrainer initialement le modèle (donc pas utilisé ici)\n",
    "            X_train, X_test, y_train, y_test = \\\n",
    "                    train_test_split(X, y, test_size=int(.2 * X.shape[0]))\n",
    "        \n",
    "            for train, validation in kfold.split(X_train, y_train): # or kfold.split(X_train, y_train):\n",
    "\n",
    "                # print(X_train[train].shape, X_train[validation].shape)\n",
    "                # print(y_train)\n",
    "\n",
    "                #Training with EarlyStopping based on the crossvalidation validation set\n",
    "                model = base_model    #clone_model(base_model)  #TODO : verifier qu'il s'agit bien d'une nouvelle instance\n",
    "                if model_name in []:\n",
    "                    model.fit(X_train[train], y_train[train], callbacks=[callback])\n",
    "                else:\n",
    "                    # No need for Early Stopping callback for GBC : https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_early_stopping.html\n",
    "                    model.fit(X_train[train], y_train[train])\n",
    "\n",
    "                # Record metrics\n",
    "                all_accuracies.append(model.score(X_train[validation], y_train[validation]))\n",
    "\n",
    "            #Save model results\n",
    "            results = np.array([[dataset_id, model_name, acc] for acc in all_accuracies])\n",
    "            df = pd.DataFrame(results, columns = ['datasetId', 'modelName', 'accuracy'])\n",
    "            save_path = f'./results/{dataset_id}-{model_name}.csv'\n",
    "            df.to_csv(save_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.DataFrame(columns = ['datasetId', 'modelName', 'accuracy'])\n",
    "avg_results = pd.DataFrame(columns = ['datasetId', 'modelName', 'accuracy'])    #Averaged accuracy of K-fold model results\n",
    "pd.DataFrame(results, columns = ['datasetId', 'modelName', 'accuracy'])\n",
    "for dataset_id in dataset_ids:\n",
    "    for model_name, base_model in models:\n",
    "\n",
    "        #Load model results\n",
    "        load_path = f'./results/{dataset_id}-{model_name}.csv'\n",
    "        df = pd.read_csv(load_path)\n",
    "\n",
    "        #Join results go global dataframes\n",
    "        all_results.append(df)\n",
    "\n",
    "        mean_acc = df['accuracy'].mean()\n",
    "        avg_results.append(\\\n",
    "            pd.DataFrame(\\\n",
    "                np.array([dataset_id, model_name, mean_acc]),\n",
    "                columns = ['datasetId', 'modelName', 'accuracy']\n",
    "                )\n",
    "            )\n",
    "\n",
    "#Saving grouped results \n",
    "save_path = f'./results/ALL-RESULTS.csv'\n",
    "all_results.to_csv(save_path, index=False)\n",
    "\n",
    "save_path = f'./results/RESULTS.csv'\n",
    "avg_results.to_csv(save_path, index=False)        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 2.7.16 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
